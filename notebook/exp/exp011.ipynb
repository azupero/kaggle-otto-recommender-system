{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948880f8-7e07-4caa-b1c5-ab7120694b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "import polars as pl\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import hashlib\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import cupy as cp\n",
    "\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d471b8-ae41-4048-8766-3991f6ceac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EXP_ID = \"011\"\n",
    "    VALIDATION = False\n",
    "    INPUT_DIR = Path(\"../../input/train_valid\") if VALIDATION else Path(\"../../input\")\n",
    "    OUTPUT_DIR = Path(f\"../../output/exp{EXP_ID}\")\n",
    "    CANDIDATE_FEATURE_DIR = OUTPUT_DIR / \"cv_feature\" if VALIDATION else OUTPUT_DIR / \"feature\"\n",
    "    DISK_PIECES = 4\n",
    "\n",
    "\n",
    "Path.mkdir(Config.OUTPUT_DIR, exist_ok=True)\n",
    "Path.mkdir(Config.CANDIDATE_FEATURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d7f899-fdcc-48e3-8b49-c37ce86159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Utils\n",
    "# =================================\n",
    "def get_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        chunk = cudf.read_parquet(path)\n",
    "        chunk[\"session\"] = chunk[\"session\"].astype(\"int32\")\n",
    "        chunk[\"aid\"] = chunk[\"aid\"].astype(\"int32\")\n",
    "        chunk[\"ts\"] = (chunk[\"ts\"] / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(type_labels).astype(\"int8\")\n",
    "        dfs.append(chunk)\n",
    "        \n",
    "    del chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    return cudf.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_pl_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        chunk = pl.read_parquet(path)\n",
    "        chunk = chunk.with_columns([\n",
    "            pl.col(\"session\").cast(pl.Int32),\n",
    "            pl.col(\"aid\").cast(pl.Int32),\n",
    "            (pl.col(\"ts\") / 1000).cast(pl.Int32),\n",
    "            pl.col(\"type\").apply(lambda x: type_labels[x]).cast(pl.Int32)\n",
    "        ])\n",
    "        dfs.append(chunk)\n",
    "    \n",
    "    return pl.concat(dfs)\n",
    "\n",
    "\n",
    "def get_gpu_memory(cmd_path=\"nvidia-smi\",\n",
    "                   target_properties=(\"memory.total\", \"memory.used\")):\n",
    "    \"\"\"\n",
    "    ref: https://www.12-technology.com/2022/01/pythongpu.html\n",
    "    Returns\n",
    "    -------\n",
    "    gpu_total : ndarray,  \"memory.total\"\n",
    "    gpu_used: ndarray, \"memory.used\"\n",
    "    \"\"\"\n",
    "\n",
    "    # format option\n",
    "    format_option = \"--format=csv,noheader,nounits\"\n",
    "\n",
    "    cmd = '%s --query-gpu=%s %s' % (cmd_path, ','.join(target_properties), format_option)\n",
    "\n",
    "    # Command execution in sub-processes\n",
    "    cmd_res = subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "    gpu_lines = cmd_res.decode().split('\\n')[0].split(', ')\n",
    "\n",
    "    gpu_total = int(gpu_lines[0]) / 1024\n",
    "    gpu_used = int(gpu_lines[1]) / 1024\n",
    "\n",
    "    gpu_total = np.round(gpu_used, 1)\n",
    "    gpu_used = np.round(gpu_used, 1)\n",
    "    return gpu_total, gpu_used\n",
    "\n",
    "\n",
    "class Trace():\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    @contextmanager\n",
    "    def timer(self, title):\n",
    "        t0 = time.time()\n",
    "        p = psutil.Process(os.getpid())\n",
    "        cpu_m0 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m0 = get_gpu_memory()[0]\n",
    "        yield\n",
    "        cpu_m1 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m1 = get_gpu_memory()[0]\n",
    "\n",
    "        cpu_delta = cpu_m1 - cpu_m0\n",
    "        if self.cuda: gpu_delta = gpu_m1 - gpu_m0\n",
    "\n",
    "        cpu_sign = '+' if cpu_delta >= 0 else '-'\n",
    "        cpu_delta = math.fabs(cpu_delta)\n",
    "\n",
    "        if self.cuda: gpu_sign = '+' if gpu_delta >= 0 else '-'\n",
    "        if self.cuda: gpu_delta = math.fabs(gpu_delta)\n",
    "\n",
    "        cpu_message = f'{cpu_m1:.1f}GB({cpu_sign}{cpu_delta:.1f}GB)'\n",
    "        if self.cuda: gpu_message = f'{gpu_m1:.1f}GB({gpu_sign}{gpu_delta:.1f}GB)'\n",
    "\n",
    "        if self.cuda:\n",
    "            message = f\"[cpu: {cpu_message}, gpu: {gpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "        else:\n",
    "            message = f\"[cpu: {cpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "\n",
    "        print(message, file=sys.stderr)\n",
    "        \n",
    "trace = Trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49523d66-107d-485c-baff-bf96e19c1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Candidate features\n",
    "# =================================\n",
    "def read_file(f):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    df = cudf.read_parquet(f)\n",
    "    df[\"session\"] = df[\"session\"].astype(\"int32\")\n",
    "    df[\"aid\"] = df[\"aid\"].astype(\"int32\")\n",
    "    df[\"ts\"] = (df[\"ts\"] / 1000).astype(\"int32\")\n",
    "    df[\"type\"] = df[\"type\"].map(type_labels).astype(\"int8\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_type_weighted_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    type_weight = {0: 1, 1: 6, 2: 3}\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df[\"n\"] = df.groupby(\"session\").cumcount()\n",
    "                df = df.loc[df[\"n\"] < 30].drop(\"n\",axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on=\"session\")\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part * SIZE) & (df.aid_x < (part+1) * SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values([\"aid_x\", \"wgt\"], ascending=[True, False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp[\"n\"] = tmp.groupby(\"aid_x\").aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp[\"n\"] < 15].drop(\"n\", axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_buy2buy_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.loc[df[\"type\"].isin([1, 2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)] # 14days\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df['wgt'] = 1\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_buy2buy_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_clicks_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', \"ts_x\", 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df[\"wgt\"] = 1 + 3 * (df[\"ts_x\"] - 1659304800) / (1662328791 - 1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp[\"n\"] < 20].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_20_clicks_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762edc17-3c35-4c18-b9d9-a80ecd7be1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 4.2GB(+3.5GB), gpu: 0.2GB(+0.0GB): 68.7sec] load all data \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load all data\"):\n",
    "    train_df = get_pl_input_data(Config.INPUT_DIR, \"train\")\n",
    "    test_df = get_pl_input_data(Config.INPUT_DIR, \"test\")\n",
    "    whole_df = pl.concat([train_df, test_df])\n",
    "    # whole_df = whole_df.with_column((pl.col(\"aid\").cast(str) + \"_\" +  pl.col(\"type\").cast(str)).alias(\"aid_type\"))\n",
    "    \n",
    "    del train_df, test_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bd9b93-3624-4dd7-803d-f8108d6f7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfxn(x):\n",
    "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
    "\n",
    "\n",
    "def get_item2vec(input_df: pl.DataFrame, type_label: int):\n",
    "    corpus = input_df.filter(pl.col(\"type\") == type_label).groupby([\"session\"]).agg(pl.col(\"aid\"))[\"aid\"].to_list()\n",
    "    item2vec = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=100,\n",
    "        window=20,\n",
    "        min_count=1,\n",
    "        sg=1,\n",
    "        hs=0,\n",
    "        hashfxn=hashfxn,\n",
    "        epochs=300,\n",
    "        seed=42,\n",
    "        workers=-1\n",
    "    )\n",
    "    item2vec.wv.save(str(Config.CANDIDATE_FEATURE_DIR / f\"item2vec_{type_label}.wordvectors\"))\n",
    "    \n",
    "    del corpus, item2vec\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "# def get_item2vec(input_df: pl.DataFrame):\n",
    "#     corpus = input_df.groupby([\"session\"]).agg(pl.col(\"aid_type\"))[\"aid_type\"].to_list()\n",
    "#     item2vec = Word2Vec(\n",
    "#         sentences=corpus,\n",
    "#         vector_size=100,\n",
    "#         window=20,\n",
    "#         min_count=1,\n",
    "#         sg=1,\n",
    "#         hs=0,\n",
    "#         hashfxn=hashfxn,\n",
    "#         epochs=100,\n",
    "#         seed=42,\n",
    "#         workers=-1\n",
    "#     )\n",
    "#     item2vec.wv.save(str(Config.CANDIDATE_FEATURE_DIR / f\"item2vec.wordvectors\"))\n",
    "    \n",
    "#     del corpus, item2vec\n",
    "#     gc.collect()\n",
    "    \n",
    "    \n",
    "# with trace.timer(\"item2vec\"):\n",
    "#     get_item2vec(whole_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf76be2b-35d6-446d-90bd-4f92d7b235c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 21.4GB(+17.2GB), gpu: 0.2GB(+0.0GB): 575.9sec] clicks item2vec \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"clicks item2vec\"):\n",
    "    get_item2vec(whole_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0ab54b-c22e-4aac-b52d-1e7bf6b879f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 11.4GB(+7.3GB), gpu: 0.2GB(+0.0GB): 418.9sec] carts item2vec \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"carts item2vec\"):\n",
    "    get_item2vec(whole_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df946fa-8d4b-4ba2-887a-2104fba17ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 6.4GB(+2.3GB), gpu: 0.2GB(+0.0GB): 210.5sec] orders item2vec \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"orders item2vec\"):\n",
    "    get_item2vec(whole_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641c749e-b31e-4921-a5c6-478218bd14de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 3.0GB(+2.3GB), gpu: 1.2GB(+1.0GB): 171.3sec] cart_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"cart_co_matirx\"):\n",
    "    get_type_weighted_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b736689c-9515-43ed-8106-cb56bd292d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 3.0GB(+0.0GB), gpu: 1.2GB(+0.0GB): 33.3sec] buy2buy_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"buy2buy_co_matirx\"):\n",
    "    get_buy2buy_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ced086d-c1c8-49fa-a9c3-37f41c784b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 3.0GB(-0.0GB), gpu: 1.2GB(+0.0GB): 166.1sec] clicks_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"clicks_co_matirx\"):\n",
    "    get_clicks_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103e4ce-69de-4785-8630-8a8461e71823",
   "metadata": {},
   "source": [
    "### Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05bf36a-8467-4cee-9ff2-b5007a8ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def parquet2dict(path: Path) -> dict:\n",
    "    df = pd.read_parquet(path)\n",
    "    return df.groupby([\"aid_x\"])[\"aid_y\"].apply(list).to_dict()\n",
    "    # return df.groupby([\"aid_x\"]).parallel_apply(lambda x: x[\"aid_y\"].to_list()).to_dict()\n",
    "\n",
    "    \n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter()\n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # USE item2vec\n",
    "    aids3 = list(itertools.chain(*[list(click_index2key[click_indices[click_model.key_to_index[aid]][1:]]) for aid in unique_aids if aid in click_model.key_to_index]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid for aid, cnt in Counter(aids2 + aids3).most_common(20) if aid not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20-len(result)]\n",
    "\n",
    "\n",
    "def suggest_carts(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']==0) | (df['type']==1)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_buys if aid in top_15_carts2orders]))\n",
    "        for aid in aids3:\n",
    "            aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_buys if aid in top_15_carts2orders]))\n",
    "    # USE item2vec\n",
    "    aids4 = list(itertools.chain(*[list(cart_index2key[cart_indices[cart_model.key_to_index[aid]][1:]]) for aid in unique_aids if aid in cart_model.key_to_index]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3 + aids4).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_carts)[:20-len(result)]\n",
    "\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']==1) | (df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "        for aid in aids3:\n",
    "            aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_aids if aid in top_15_carts2orders]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "    # USE item2vec\n",
    "    aids4 = list(itertools.chain(*[list(order_index2key[order_indices[order_model.key_to_index[aid]][1:]]) for aid in unique_aids if aid in order_model.key_to_index]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3 + aids4).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e48022e-41da-439d-8ad0-2b603e35ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "# N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "\n",
    "# def df_parallelize_run(func, t_split):\n",
    "    \n",
    "#     num_cores = np.min([N_CORES, len(t_split)])\n",
    "#     pool = Pool(num_cores)\n",
    "#     df = pool.map(func, t_split)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# def suggest_clicks(df):\n",
    "#     # USE USER HISTORY AIDS AND TYPES\n",
    "#     aids = df[\"aid\"].to_list()\n",
    "#     types = df[\"type\"].to_list()\n",
    "#     session = df[\"session\"].unique()[0]\n",
    "#     # UNIQUE AIDS\n",
    "#     unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     # df = df.loc[(df['type']==0)]\n",
    "#     # unique_clicks = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "#     # # USE USER HISTORY AIDS AND TYPES\n",
    "#     # session = df[0]\n",
    "#     # aids = df[1]\n",
    "#     # types = df[2]\n",
    "#     # # UNIQUE AIDS\n",
    "#     # unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     # unique_clicks = list(dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [0]][::-1]))\n",
    "    \n",
    "#     # RERANK CANDIDATES USING WEIGHTS\n",
    "#     if len(unique_aids) >= 20:\n",
    "#         weights = np.logspace(0.1, 1, len(aids),base=2, endpoint=True) - 1\n",
    "#         aids_temp = Counter()\n",
    "#         # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "#         for aid, w, t in zip(aids, weights, types): \n",
    "#             aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "#         sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        \n",
    "#         return sorted_aids\n",
    "    \n",
    "#     if session in click_session2index:\n",
    "#         # USE item2vec\n",
    "#         aids2 = index2key[click_indices[click_session2index[session]]]\n",
    "#         # aids2 = list(aids2)\n",
    "#         aids2 = [aid.split(\"_\")[0] for aid in aids2 if int(aid.split(\"_\")[1]) in [0, 1, 2]]\n",
    "#         # RERANK CANDIDATES\n",
    "#         top_aids2 = [aid2 for aid2 in aids2 if aid2 not in unique_aids]    \n",
    "#         result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "#     else:\n",
    "#         result = []\n",
    "    \n",
    "#     # USE TOP20 TEST CLICKS\n",
    "#     return result + list(top_clicks)[:20 - len(result)]\n",
    "\n",
    "\n",
    "# def suggest_carts(df):\n",
    "#     # USE USER HISTORY AIDS AND TYPES\n",
    "#     aids = df[\"aid\"].to_list()\n",
    "#     types = df[\"type\"].to_list()\n",
    "#     session = df[\"session\"].unique()[0]\n",
    "#     # UNIQUE AIDS\n",
    "#     unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     df = df.loc[(df['type'] == 0) | (df['type'] == 1)]\n",
    "#     unique_carts = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "#     # # USE USER HISTORY AIDS AND TYPES\n",
    "#     # session = df[0]\n",
    "#     # aids = df[1]\n",
    "#     # types = df[2]\n",
    "#     # # UNIQUE AIDS\n",
    "#     # unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     # unique_carts = list(dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1]][::-1]))\n",
    "#     # RERANK CANDIDATES USING WEIGHTS\n",
    "#     if len(unique_aids) >= 20:\n",
    "#         weights = np.logspace(0.1, 1, len(aids),base=2, endpoint=True) - 1\n",
    "#         aids_temp = Counter()\n",
    "#         # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "#         for aid, w, t in zip(aids, weights, types): \n",
    "#             aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "#         sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        \n",
    "#         return sorted_aids\n",
    "    \n",
    "#     if session in cart_session2index:\n",
    "#         # USE item2vec\n",
    "#         aids2 = index2key[cart_indices[cart_session2index[session]]]\n",
    "#         # aids2 = list(aids2)\n",
    "#         aids2 = [aid.split(\"_\")[0] for aid in aids2 if int(aid.split(\"_\")[1]) in [0, 1]]\n",
    "#         # RERANK CANDIDATES\n",
    "#         top_aids2 = [aid2 for aid2 in aids2 if aid2 not in unique_carts]    \n",
    "#         result = unique_carts + top_aids2[:20 - len(unique_carts)]\n",
    "#     else:\n",
    "#         result = []\n",
    "    \n",
    "#     # USE TOP20 TEST CLICKS\n",
    "#     return result + list(top_carts)[:20 - len(result)]\n",
    "\n",
    "\n",
    "# def suggest_orders(df):\n",
    "#     # USE USER HISTORY AIDS AND TYPES\n",
    "#     aids = df[\"aid\"].to_list()\n",
    "#     types = df[\"type\"].to_list()\n",
    "#     session = df[\"session\"].unique()[0]\n",
    "#     # UNIQUE AIDS\n",
    "#     unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     df = df.loc[(df['type'] == 1) | (df['type'] == 2)]\n",
    "#     unique_orders = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "#     # # USE USER HISTORY AIDS AND TYPES\n",
    "#     # session = df[0]\n",
    "#     # aids = df[1]\n",
    "#     # types = df[2]\n",
    "#     # # UNIQUE AIDS\n",
    "#     # unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "#     # unique_orders = list(dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [2]][::-1]))\n",
    "#     # RERANK CANDIDATES USING WEIGHTS\n",
    "#     if len(unique_aids) >= 20:\n",
    "#         weights = np.logspace(0.1, 1, len(aids),base=2, endpoint=True) - 1\n",
    "#         aids_temp = Counter()\n",
    "#         # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "#         for aid, w, t in zip(aids, weights, types): \n",
    "#             aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "#         sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        \n",
    "#         return sorted_aids\n",
    "    \n",
    "#     if session in order_session2index:\n",
    "#         # USE item2vec\n",
    "#         aids2 = index2key[order_indices[order_session2index[session]]]\n",
    "#         # aids2 = list(aids2)\n",
    "#         aids2 = [aid.split(\"_\")[0] for aid in aids2 if int(aid.split(\"_\")[1]) in [1, 2]]\n",
    "#         # RERANK CANDIDATES\n",
    "#         top_aids2 = [aid2 for aid2 in aids2 if aid2 not in unique_orders]    \n",
    "#         result = unique_orders + top_aids2[:20 - len(unique_orders)]\n",
    "#     else:\n",
    "#         result = []\n",
    "    \n",
    "#     # USE TOP20 TEST CLICKS\n",
    "#     return result + list(top_orders)[:20 - len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b77ccf-69ac-4956-ac97-61f2f46a4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_similar_items_by_item2vec(item2vec_model, session_type):\n",
    "#     input_df = get_input_data(Config.INPUT_DIR, \"test\")\n",
    "#     # last 30 events per session\n",
    "#     input_df = input_df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False])\n",
    "#     input_df[\"n\"] = input_df.groupby([\"session\"]).cumcount()\n",
    "#     input_df = input_df.loc[input_df[\"n\"] < 30].reset_index(drop=True)\n",
    "#     # item2vecに含まれるaidにfintering\n",
    "#     input_df[\"aid_type\"] = input_df[\"aid\"].astype(str) + \"_\" + input_df[\"type\"].astype(str)\n",
    "#     input_df = input_df.loc[input_df[\"aid_type\"].isin(item2vec_model.index_to_key)].reset_index(drop=True)\n",
    "#     # input_df = input_df.loc[input_df[\"aid\"].isin(item2vec_model.index_to_key)].reset_index(drop=True)\n",
    "\n",
    "#     # sessionにtype == clickがないイベントログ\n",
    "#     input_df[\"is_type\"] = input_df[\"type\"].isin(session_type).astype(\"int8\")\n",
    "#     input_df[\"count_type\"] = input_df.groupby(\"session\")[\"is_type\"].transform(\"sum\")\n",
    "\n",
    "#     output_df = input_df.loc[(input_df[\"type\"].isin(session_type)) & (input_df[\"count_type\"] > 0)]\n",
    "#     output_df = cudf.concat([output_df, input_df.loc[input_df[\"count_type\"] == 0]], ignore_index=True)\n",
    "#     output_df = output_df.sort_values(by=[\"session\", \"ts\"]).reset_index(drop=True).drop(columns=[\"is_type\", \"count_type\"])\n",
    "    \n",
    "#     session_idx = np.cumsum(output_df.groupby([\"session\"], sort=True).size().to_numpy())\n",
    "#     session2index = {k: v for v, k in enumerate(sorted(output_df[\"session\"].unique().to_pandas().to_list()))}\n",
    "    \n",
    "#     # session embedding\n",
    "#     session_embeddings = item2vec_model[output_df[\"aid_type\"].to_numpy()]\n",
    "#     # session_embeddings = item2vec_model[output_df[\"aid\"].to_numpy()]\n",
    "#     session_embeddings = np.split(session_embeddings, session_idx[:-1])\n",
    "#     session_embeddings = np.array([np.mean(i, axis=0) for i in session_embeddings])\n",
    "    \n",
    "#     # most similars\n",
    "#     model = NearestNeighbors(n_neighbors=20, metric=\"cosine\", output_type=\"numpy\")\n",
    "#     model.fit(item2vec_model.vectors)\n",
    "    \n",
    "#     _, indices = model.kneighbors(session_embeddings)\n",
    "    \n",
    "#     del input_df, output_df, session_embeddings, model\n",
    "#     gc.collect()\n",
    "    \n",
    "#     return session2index, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14751a2d-686e-4e45-9319-16b4f02f9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_items(word2vec):\n",
    "    X = cp.array(word2vec.vectors)\n",
    "    model = NearestNeighbors(n_neighbors=21, metric=\"cosine\", output_type=\"numpy\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    _, indices = model.kneighbors(X)\n",
    "    \n",
    "    del X, model\n",
    "    gc.collect()\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a991f4-a5b3-4123-a1d5-023d8ee88498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 3.1GB(+2.5GB), gpu: 1.2GB(+1.0GB): 10.6sec] load test set \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load test set\"):\n",
    "    test_df = get_input_data(Config.INPUT_DIR, \"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7be783-02b8-46ee-8b42-64e1f9520998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 12.0GB(+8.9GB), gpu: 1.5GB(+0.3GB): 277.3sec] load candidate feature \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load candidate feature\"):\n",
    "    # type_weighted\n",
    "    top_15_carts2orders = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_carts_orders_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_15_carts2orders.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{i}.pqt\"))\n",
    "    \n",
    "    # buy2buy\n",
    "    top_15_buy2buy = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_buy2buy_0.pqt\")\n",
    "    \n",
    "    # clicks\n",
    "    top_20_clicks = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_20_clicks_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_20_clicks.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_20_clicks_{i}.pqt\"))\n",
    "        \n",
    "    top_clicks = test_df.loc[test_df[\"type\"] == 0, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    top_carts = test_df.loc[test_df[\"type\"] == 1, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    top_orders = test_df.loc[test_df[\"type\"] == 2, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    \n",
    "    # clicks\n",
    "    click_model = KeyedVectors.load(str(Config.CANDIDATE_FEATURE_DIR / \"item2vec_0.wordvectors\"), mmap=\"r\")\n",
    "    click_index2key = np.array(click_model.index_to_key)\n",
    "    click_indices = get_similar_items(click_model)\n",
    "    \n",
    "    # carts\n",
    "    cart_model = KeyedVectors.load(str(Config.CANDIDATE_FEATURE_DIR / \"item2vec_1.wordvectors\"), mmap=\"r\")\n",
    "    cart_index2key = np.array(cart_model.index_to_key)\n",
    "    cart_indices = get_similar_items(cart_model)\n",
    "    \n",
    "    # orders\n",
    "    order_model = KeyedVectors.load(str(Config.CANDIDATE_FEATURE_DIR / \"item2vec_2.wordvectors\"), mmap=\"r\")\n",
    "    order_index2key = np.array(order_model.index_to_key)\n",
    "    order_indices = get_similar_items(order_model)\n",
    "    \n",
    "    # all\n",
    "    # item2vec = KeyedVectors.load(str(Config.CANDIDATE_FEATURE_DIR / \"item2vec.wordvectors\"), mmap=\"r\")\n",
    "    # index2key = np.array(item2vec.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1aecb0-27b2-4c9e-bb72-c2bcaf9d2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with trace.timer(\"load test set\"):\n",
    "#     temp = [group for name, group in test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"])]\n",
    "#     test_list = [[i[\"session\"].iloc[0], i[\"aid\"].to_list(), i[\"type\"].to_list()] for i in temp]\n",
    "    \n",
    "#     del temp, test_df\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9caa3a8-1722-4130-ae06-ec786ff89385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 12.6GB(+0.6GB), gpu: 1.5GB(+0.0GB): 167.2sec] click inference \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"click inference\"):\n",
    "    # click_session2index, click_indices = get_similar_items_by_item2vec(item2vec, [0, 1, 2])\n",
    "    # temp = df_parallelize_run(suggest_clicks, test_list)\n",
    "    # pred_clicks = pd.Series([f[1] for f in temp], index=[f[0] for f in temp])\n",
    "    pred_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_clicks)\n",
    "    \n",
    "    #del click_session2index, click_indices, click_model, click_index2key #temp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1387c53-ac67-4250-804d-33dcab274a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 13.2GB(+0.6GB), gpu: 1.5GB(+0.0GB): 751.6sec] cart inference \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"cart inference\"):\n",
    "    # cart_session2index, cart_indices = get_similar_items_by_item2vec(item2vec, [0, 1])\n",
    "    # temp = df_parallelize_run(suggest_carts, test_list)\n",
    "    # pred_carts = pd.Series([f[1] for f in temp], index=[f[0] for f in temp])\n",
    "    pred_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_carts)\n",
    "    \n",
    "    #del cart_session2index, cart_indices, #cart_model, cart_index2key #temp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c353e33-659d-44f9-8425-38465ef6f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 13.8GB(+0.6GB), gpu: 1.5GB(+0.0GB): 700.7sec] order inference \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"order inference\"):\n",
    "    # order_session2index, order_indices = get_similar_items_by_item2vec(item2vec, [1, 2])\n",
    "    # temp = df_parallelize_run(suggest_orders, test_list)\n",
    "    # pred_orders = pd.Series([f[1] for f in temp], index=[f[0] for f in temp])\n",
    "    pred_orders = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_buys)\n",
    "    \n",
    "    #del order_session2index, order_indices, #order_model, order_index2key #temp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b678d5fa-8e1a-4c5c-8d41-e7b3cf84a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 15.2GB(+1.4GB), gpu: 1.5GB(+0.0GB): 37.1sec] make pred_df \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 438191 731692 1790770 942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 1502122 889686 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 7594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>834354 595994 740494 889671 987399 779477 1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 438191 731692 1790770 942...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 1502122 889686 17...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 7594...\n",
       "3  12899782_clicks  834354 595994 740494 889671 987399 779477 1344...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with trace.timer(\"make pred_df\"):\n",
    "    pred_clicks_df = pd.DataFrame(pred_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_carts_df = pd.DataFrame(pred_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_orders_df = pd.DataFrame(pred_orders.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_df = pd.concat([pred_clicks_df, pred_carts_df, pred_orders_df], axis=0).reset_index(drop=True)\n",
    "    pred_df.columns = [\"session_type\", \"labels\"]\n",
    "    pred_df[\"labels\"] = pred_df[\"labels\"].apply(lambda x: \" \".join(map(str, x)))\n",
    "    pred_df.to_csv(\n",
    "        Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_validation.csv\" if Config.VALIDATION else Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_sub.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de360c3a-10f7-4f9a-8e02-a9a6e3f7592e",
   "metadata": {},
   "source": [
    "### validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6ac19-4336-4cb6-9db8-3f2e74f51327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with trace.timer(\"delete temp file\"):\n",
    "    del test_df\n",
    "    del top_15_carts2orders, top_15_buy2buy, top_20_clicks, top_clicks, top_carts, top_orders\n",
    "    del pred_clicks, pred_buys\n",
    "    del pred_clicks_df, pred_carts_df, pred_orders_df\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8596a73d-fe5f-4fe7-921c-1800d2ab82c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.523267977316038\n",
      "carts recall = 0.4093993163812367\n",
      "orders recall = 0.6516749489393866\n",
      "=============\n",
      "Overall Recall = 0.5661515620096067\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 16.6GB(+0.8GB), gpu: 1.3GB(+0.0GB): 137.6sec] compute cv score \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"compute cv score\"):\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet(Config.INPUT_DIR / \"test_labels.parquet\")\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "    \n",
    "    del sub, test_labels, recall, score\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb40280-94dd-4b73-8f94-208b59119237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
