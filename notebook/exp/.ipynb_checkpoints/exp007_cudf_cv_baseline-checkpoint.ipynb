{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948880f8-7e07-4caa-b1c5-ab7120694b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import polars as pl\n",
    "import cudf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eddc452-dc42-451f-9e4c-77d8d22599e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "INPUT_DIR = Path(\"../../input\")\n",
    "EXP_ID = \"007\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d7f899-fdcc-48e3-8b49-c37ce86159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Utils\n",
    "# =================================\n",
    "def get_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        chunk = cudf.read_parquet(path)\n",
    "        chunk[\"session\"] = chunk[\"session\"].astype(\"int32\")\n",
    "        chunk[\"aid\"] = chunk[\"aid\"].astype(\"int32\")\n",
    "        chunk[\"ts\"] = (chunk[\"ts\"] / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(type_labels).astype(\"int8\")\n",
    "        dfs.append(chunk)\n",
    "    \n",
    "    return cudf.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_gpu_memory(cmd_path=\"nvidia-smi\",\n",
    "                   target_properties=(\"memory.total\", \"memory.used\")):\n",
    "    \"\"\"\n",
    "    ref: https://www.12-technology.com/2022/01/pythongpu.html\n",
    "    Returns\n",
    "    -------\n",
    "    gpu_total : ndarray,  \"memory.total\"\n",
    "    gpu_used: ndarray, \"memory.used\"\n",
    "    \"\"\"\n",
    "\n",
    "    # format option\n",
    "    format_option = \"--format=csv,noheader,nounits\"\n",
    "\n",
    "    cmd = '%s --query-gpu=%s %s' % (cmd_path, ','.join(target_properties), format_option)\n",
    "\n",
    "    # Command execution in sub-processes\n",
    "    cmd_res = subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "    gpu_lines = cmd_res.decode().split('\\n')[0].split(', ')\n",
    "\n",
    "    gpu_total = int(gpu_lines[0]) / 1024\n",
    "    gpu_used = int(gpu_lines[1]) / 1024\n",
    "\n",
    "    gpu_total = np.round(gpu_used, 1)\n",
    "    gpu_used = np.round(gpu_used, 1)\n",
    "    return gpu_total, gpu_used\n",
    "\n",
    "\n",
    "class Trace():\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    @contextmanager\n",
    "    def timer(self, title):\n",
    "        t0 = time.time()\n",
    "        p = psutil.Process(os.getpid())\n",
    "        cpu_m0 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m0 = get_gpu_memory()[0]\n",
    "        yield\n",
    "        cpu_m1 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m1 = get_gpu_memory()[0]\n",
    "\n",
    "        cpu_delta = cpu_m1 - cpu_m0\n",
    "        if self.cuda: gpu_delta = gpu_m1 - gpu_m0\n",
    "\n",
    "        cpu_sign = '+' if cpu_delta >= 0 else '-'\n",
    "        cpu_delta = math.fabs(cpu_delta)\n",
    "\n",
    "        if self.cuda: gpu_sign = '+' if gpu_delta >= 0 else '-'\n",
    "        if self.cuda: gpu_delta = math.fabs(gpu_delta)\n",
    "\n",
    "        cpu_message = f'{cpu_m1:.1f}GB({cpu_sign}{cpu_delta:.1f}GB)'\n",
    "        if self.cuda: gpu_message = f'{gpu_m1:.1f}GB({gpu_sign}{gpu_delta:.1f}GB)'\n",
    "\n",
    "        if self.cuda:\n",
    "            message = f\"[cpu: {cpu_message}, gpu: {gpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "        else:\n",
    "            message = f\"[cpu: {cpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "\n",
    "        print(message, file=sys.stderr)\n",
    "        \n",
    "trace = Trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d471b8-ae41-4048-8766-3991f6ceac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(\"../../input\")\n",
    "files = sorted(list(INPUT_DIR.glob(\"train_parquet/*.parquet\")))\n",
    "CHUNK = int(np.ceil(len(files)/6))\n",
    "READ_CT = 5\n",
    "type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49523d66-107d-485c-baff-bf96e19c1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-visitation matrix\n",
    "def read_file(f):\n",
    "    df = cudf.read_parquet(f)\n",
    "    df[\"session\"] = df[\"session\"].astype(\"int32\")\n",
    "    df[\"aid\"] = df[\"aid\"].astype(\"int32\")\n",
    "    df[\"ts\"] = (df[\"ts\"] / 1000).astype(\"int32\")\n",
    "    df[\"type\"] = df[\"type\"].map(type_labels).astype(\"int8\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_type_weighted_co_visitation_matrix():\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    type_weight = {0: 1, 1: 6, 2: 3}\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_carts_orders_exp{EXP_ID}_{part}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641c749e-b31e-4921-a5c6-478218bd14de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 21 in groups of 5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_120/135944133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cart_co_matirx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mget_type_weighted_co_visitation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_120/16975263.py\u001b[0m in \u001b[0;36mget_type_weighted_co_visitation_matrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREAD_CT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_120/16975263.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# co-visitation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"session_int32\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"session\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aid_int32\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(filepath_or_buffer, engine, columns, filters, row_groups, strings_to_categorical, use_pandas_metadata, use_python_file_object, categorical_partitions, open_file_options, *args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     return _parquet_to_frame(\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mfilepaths_or_buffers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/io/parquet.py\u001b[0m in \u001b[0;36m_parquet_to_frame\u001b[0;34m(paths_or_buffers, row_groups, partition_keys, partition_categories, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;31m# one call to `_read_parquet`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpartition_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         return _read_parquet(\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0mpaths_or_buffers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/io/parquet.py\u001b[0m in \u001b[0;36m_read_parquet\u001b[0;34m(filepaths_or_buffers, engine, columns, row_groups, strings_to_categorical, use_pandas_metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;31m# cudf and pyarrow to read parquet data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cudf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         return libparquet.read_parquet(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mfilepaths_or_buffers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mparquet.pyx\u001b[0m in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mparquet.pyx\u001b[0m in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m def loads(s, *, cls=None, object_hook=None, parse_float=None,\n\u001b[0m\u001b[1;32m    300\u001b[0m         parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n\u001b[1;32m    301\u001b[0m     \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trace.timer(\"cart_co_matirx\"):\n",
    "    get_type_weighted_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad6419d-ef98-4d24-abe6-ed8ad351c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buy2buy_co_visitation_matrix():\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.loc[df[\"type\"].isin([1, 2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)] # 14days\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_buy2buy_exp{EXP_ID}_{part}.pqt')\n",
    "        \n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0678737-5618-412b-ab4b-ad0ce118669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 21 in groups of 5...\n",
      "Processing files 22 thru 43 in groups of 5...\n",
      "Processing files 44 thru 65 in groups of 5...\n",
      "Processing files 66 thru 87 in groups of 5...\n",
      "Processing files 88 thru 109 in groups of 5...\n",
      "Processing files 110 thru 128 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.4GB(+0.9GB), gpu: 2.4GB(+2.1GB): 47.4sec] buy_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"buy_co_matirx\"):\n",
    "    mat = get_buy2buy_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cb88cf-318f-4d61-bf7b-6b76900825bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clicks_co_visitation_matrix():\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df[\"wgt\"] = 1 + 3 * (df[\"ts_x\"] - 1659304800) / (1662328791 - 1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_clicks_exp{EXP_ID}_{part}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ee0963-7828-404d-b992-e531d8274914",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_clicks_co_visitation_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_120/2363920849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"click_co_matirx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mget_clicks_co_visitation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_clicks_co_visitation_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"click_co_matirx\"):\n",
    "    get_clicks_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103e4ce-69de-4785-8630-8a8461e71823",
   "metadata": {},
   "source": [
    "### Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05bf36a-8467-4cee-9ff2-b5007a8ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d22116-d9bd-4880-9e40-cbf2ac274d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polars_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        df = (\n",
    "            pl.read_parquet(path)\n",
    "            .with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int32), \n",
    "                pl.col(\"aid\").cast(pl.Int32), \n",
    "                (pl.col(\"ts\") / 1000).cast(pl.Int32),\n",
    "                pl.col(\"type\").apply(lambda x: type_labels[x]).cast(pl.Int8)\n",
    "            ]\n",
    "            )\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pl.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a991f4-a5b3-4123-a1d5-023d8ee88498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.5GB(+0.1GB), gpu: 0.8GB(+0.0GB): 2.1sec] load test set \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load test set\"):\n",
    "    test_df = get_input_data(INPUT_DIR, \"test\")\n",
    "    test_df = test_df.to_pandas()\n",
    "    # test_df = get_polars_input_data(INPUT_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7be783-02b8-46ee-8b42-64e1f9520998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_20_clicks = mat.to_pandas().groupby(\"aid_x\")[\"aid_y\"].apply(list).to_dict()\n",
    "top_clicks = test_df.loc[test_df[\"type\"] == 0, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd14e37-b772-42f7-a2a6-de5176aafb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bc261d-76f8-4f89-948c-28d72dec56c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.9/site-packages/pandarallel/data_types/dataframe_groupby.py:18: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  iterator = iter(dataframe_groupby)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcafaf32d34f33a5ee9bcb26060088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=208976), Label(value='0 / 208976')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 4.2GB(+1.9GB), gpu: 0.8GB(+0.0GB): 141.0sec] parallel apply \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"parallel apply\"):\n",
    "    pred = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(suggest_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b678d5fa-8e1a-4c5c-8d41-e7b3cf84a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>[59625, 1460571, 485256, 108125, 986164, 15512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>[1142000, 736515, 973453, 582732, 487136, 7605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>[918667, 199008, 194067, 57315, 141736, 295859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>[834354, 595994, 740494, 889671, 987399, 77947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>[1817895, 607638, 1754419, 1216820, 1729553, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671798</th>\n",
       "      <td>14571577_clicks</td>\n",
       "      <td>[1141710, 1276792, 43517, 60347, 140522, 24273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671799</th>\n",
       "      <td>14571578_clicks</td>\n",
       "      <td>[519105, 6523, 70712, 131884, 184410, 188503, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671800</th>\n",
       "      <td>14571579_clicks</td>\n",
       "      <td>[739876, 1550479, 1209992, 785544, 1750859, 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671801</th>\n",
       "      <td>14571580_clicks</td>\n",
       "      <td>[202353, 14540, 32322, 74627, 81251, 154774, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671802</th>\n",
       "      <td>14571581_clicks</td>\n",
       "      <td>[1100210, 1684953, 1124107, 462056, 1072049, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1671803 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 session                                             labels\n",
       "0        12899779_clicks  [59625, 1460571, 485256, 108125, 986164, 15512...\n",
       "1        12899780_clicks  [1142000, 736515, 973453, 582732, 487136, 7605...\n",
       "2        12899781_clicks  [918667, 199008, 194067, 57315, 141736, 295859...\n",
       "3        12899782_clicks  [834354, 595994, 740494, 889671, 987399, 77947...\n",
       "4        12899783_clicks  [1817895, 607638, 1754419, 1216820, 1729553, 3...\n",
       "...                  ...                                                ...\n",
       "1671798  14571577_clicks  [1141710, 1276792, 43517, 60347, 140522, 24273...\n",
       "1671799  14571578_clicks  [519105, 6523, 70712, 131884, 184410, 188503, ...\n",
       "1671800  14571579_clicks  [739876, 1550479, 1209992, 785544, 1750859, 50...\n",
       "1671801  14571580_clicks  [202353, 14540, 32322, 74627, 81251, 154774, 2...\n",
       "1671802  14571581_clicks  [1100210, 1684953, 1124107, 462056, 1072049, 6...\n",
       "\n",
       "[1671803 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2917681-5f02-4e19-bf2e-d8dfa6553f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
