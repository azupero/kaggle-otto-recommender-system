{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948880f8-7e07-4caa-b1c5-ab7120694b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d471b8-ae41-4048-8766-3991f6ceac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EXP_ID = \"007\"\n",
    "    VALIDATION = False\n",
    "    INPUT_DIR = Path(\"../../input/train_valid\") if VALIDATION else Path(\"../../input\")\n",
    "    OUTPUT_DIR = Path(f\"../../output/exp{EXP_ID}\")\n",
    "    CANDIDATE_FEATURE_DIR = OUTPUT_DIR / \"cv_feature\" if VALIDATION else OUTPUT_DIR / \"feature\"\n",
    "    DISK_PIECES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d7f899-fdcc-48e3-8b49-c37ce86159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Utils\n",
    "# =================================\n",
    "def get_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        chunk = cudf.read_parquet(path)\n",
    "        chunk[\"session\"] = chunk[\"session\"].astype(\"int32\")\n",
    "        chunk[\"aid\"] = chunk[\"aid\"].astype(\"int32\")\n",
    "        chunk[\"ts\"] = (chunk[\"ts\"] / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(type_labels).astype(\"int8\")\n",
    "        dfs.append(chunk)\n",
    "    \n",
    "    return cudf.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_gpu_memory(cmd_path=\"nvidia-smi\",\n",
    "                   target_properties=(\"memory.total\", \"memory.used\")):\n",
    "    \"\"\"\n",
    "    ref: https://www.12-technology.com/2022/01/pythongpu.html\n",
    "    Returns\n",
    "    -------\n",
    "    gpu_total : ndarray,  \"memory.total\"\n",
    "    gpu_used: ndarray, \"memory.used\"\n",
    "    \"\"\"\n",
    "\n",
    "    # format option\n",
    "    format_option = \"--format=csv,noheader,nounits\"\n",
    "\n",
    "    cmd = '%s --query-gpu=%s %s' % (cmd_path, ','.join(target_properties), format_option)\n",
    "\n",
    "    # Command execution in sub-processes\n",
    "    cmd_res = subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "    gpu_lines = cmd_res.decode().split('\\n')[0].split(', ')\n",
    "\n",
    "    gpu_total = int(gpu_lines[0]) / 1024\n",
    "    gpu_used = int(gpu_lines[1]) / 1024\n",
    "\n",
    "    gpu_total = np.round(gpu_used, 1)\n",
    "    gpu_used = np.round(gpu_used, 1)\n",
    "    return gpu_total, gpu_used\n",
    "\n",
    "\n",
    "class Trace():\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    @contextmanager\n",
    "    def timer(self, title):\n",
    "        t0 = time.time()\n",
    "        p = psutil.Process(os.getpid())\n",
    "        cpu_m0 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m0 = get_gpu_memory()[0]\n",
    "        yield\n",
    "        cpu_m1 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m1 = get_gpu_memory()[0]\n",
    "\n",
    "        cpu_delta = cpu_m1 - cpu_m0\n",
    "        if self.cuda: gpu_delta = gpu_m1 - gpu_m0\n",
    "\n",
    "        cpu_sign = '+' if cpu_delta >= 0 else '-'\n",
    "        cpu_delta = math.fabs(cpu_delta)\n",
    "\n",
    "        if self.cuda: gpu_sign = '+' if gpu_delta >= 0 else '-'\n",
    "        if self.cuda: gpu_delta = math.fabs(gpu_delta)\n",
    "\n",
    "        cpu_message = f'{cpu_m1:.1f}GB({cpu_sign}{cpu_delta:.1f}GB)'\n",
    "        if self.cuda: gpu_message = f'{gpu_m1:.1f}GB({gpu_sign}{gpu_delta:.1f}GB)'\n",
    "\n",
    "        if self.cuda:\n",
    "            message = f\"[cpu: {cpu_message}, gpu: {gpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "        else:\n",
    "            message = f\"[cpu: {cpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "\n",
    "        print(message, file=sys.stderr)\n",
    "        \n",
    "trace = Trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49523d66-107d-485c-baff-bf96e19c1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Candidate features\n",
    "# =================================\n",
    "def read_file(f):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    df = cudf.read_parquet(f)\n",
    "    df[\"session\"] = df[\"session\"].astype(\"int32\")\n",
    "    df[\"aid\"] = df[\"aid\"].astype(\"int32\")\n",
    "    df[\"ts\"] = (df[\"ts\"] / 1000).astype(\"int32\")\n",
    "    df[\"type\"] = df[\"type\"].map(type_labels).astype(\"int8\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_type_weighted_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"train_parquet/*.parquet\")))\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    type_weight = {0: 1, 1: 6, 2: 3}\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_buy2buy_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"train_parquet/*.parquet\")))\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.loc[df[\"type\"].isin([1, 2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)] # 14days\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_buy2buy_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_clicks_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"train_parquet/*.parquet\")))\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', \"ts_x\", 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df[\"wgt\"] = 1 + 3 * (df[\"ts_x\"] - 1659304800) / (1662328791 - 1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_clicks_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c749e-b31e-4921-a5c6-478218bd14de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 21 in groups of 5...\n",
      "Processing files 22 thru 43 in groups of 5...\n",
      "Processing files 44 thru 65 in groups of 5...\n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"cart_co_matirx\"):\n",
    "    get_type_weighted_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736689c-9515-43ed-8106-cb56bd292d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"buy2buy_co_matirx\"):\n",
    "    get_buy2buy_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced086d-c1c8-49fa-a9c3-37f41c784b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"clicks_co_matirx\"):\n",
    "    get_clicks_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103e4ce-69de-4785-8630-8a8461e71823",
   "metadata": {},
   "source": [
    "### Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bf36a-8467-4cee-9ff2-b5007a8ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def parquet2dict(path: Path) -> dict:\n",
    "    df = pd.read_parquet(path)\n",
    "    return df.groupby([\"aid_x\"])[\"aid_y\"].apply(list).to_dict()\n",
    "    # return df.groupby([\"aid_x\"]).parallel_apply(lambda x: x[\"aid_y\"].to_list()).to_dict()\n",
    "\n",
    "    \n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter()\n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_15_clicks[aid] for aid in unique_aids if aid in top_15_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20-len(result)]\n",
    "\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']==1) | (df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "        for aid in aids3:\n",
    "            aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_aids if aid in top_15_carts2orders]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a991f4-a5b3-4123-a1d5-023d8ee88498",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"load test set\"):\n",
    "    test_df = get_input_data(Config.INPUT_DIR, \"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7be783-02b8-46ee-8b42-64e1f9520998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with trace.timer(\"load candidate feature\"):\n",
    "    # type_weighted\n",
    "    top_15_carts2orders = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_carts_orders_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_15_carts2orders.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{i}.pqt\"))\n",
    "    \n",
    "    # buy2buy\n",
    "    top_15_buy2buy = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_buy2buy_0.pqt\")\n",
    "    \n",
    "    # clicks\n",
    "    top_15_clicks = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_clicks_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_15_clicks.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_15_clicks_{i}.pqt\"))\n",
    "        \n",
    "    top_clicks = test_df.loc[test_df[\"type\"] == 0, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    top_orders = test_df.loc[test_df[\"type\"] == 2, \"aid\"].value_counts().index.to_numpy()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc261d-76f8-4f89-948c-28d72dec56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"parallel apply\"):\n",
    "    pred_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(suggest_clicks)\n",
    "    pred_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(suggest_buys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678d5fa-8e1a-4c5c-8d41-e7b3cf84a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"make pred_df\"):\n",
    "    pred_clicks_df = pd.DataFrame(pred_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_carts_df = pd.DataFrame(pred_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_orders_df = pd.DataFrame(pred_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_df = pd.concat([pred_clicks_df, pred_carts_df, pred_orders_df], axis=0).reset_index(drop=True)\n",
    "    pred_df.columns = [\"session_type\", \"labels\"]\n",
    "    pred_df[\"labels\"] = pred_df[\"labels\"].apply(lambda x: \" \".join(map(str, x)))\n",
    "    pred_df.to_csv(\n",
    "        Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_validation.csv\" if Config.VALIDATION else Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_sub.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de360c3a-10f7-4f9a-8e02-a9a6e3f7592e",
   "metadata": {},
   "source": [
    "### validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6ac19-4336-4cb6-9db8-3f2e74f51327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with trace.timer(\"delete temp file\"):\n",
    "    del test_df\n",
    "    del top_15_carts2orders, top_15_buy2buy, top_15_clicks, top_clicks, top_orders\n",
    "    del pred_clicks, pred_buys\n",
    "    del pred_clicks_df, pred_carts_df, pred_orders_df\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596a73d-fe5f-4fe7-921c-1800d2ab82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace.timer(\"compute cv score\"):\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet(Config.INPUT_DIR / \"test_labels.parquet\")\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "    \n",
    "    del sub, test_labels, recall, score\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588f510-f9ed-4de3-9065-f2d8a52ae9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
