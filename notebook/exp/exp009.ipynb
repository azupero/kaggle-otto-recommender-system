{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948880f8-7e07-4caa-b1c5-ab7120694b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=False)\n",
    "\n",
    "\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d471b8-ae41-4048-8766-3991f6ceac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EXP_ID = \"009\"\n",
    "    VALIDATION = True\n",
    "    INPUT_DIR = Path(\"../../input/train_valid\") if VALIDATION else Path(\"../../input\")\n",
    "    OUTPUT_DIR = Path(f\"../../output/exp{EXP_ID}\")\n",
    "    CANDIDATE_FEATURE_DIR = OUTPUT_DIR / \"cv_feature\" if VALIDATION else OUTPUT_DIR / \"feature\"\n",
    "    DISK_PIECES = 4\n",
    "\n",
    "\n",
    "Path.mkdir(Config.OUTPUT_DIR, exist_ok=True)\n",
    "Path.mkdir(Config.CANDIDATE_FEATURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d7f899-fdcc-48e3-8b49-c37ce86159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Utils\n",
    "# =================================\n",
    "def get_input_data(input_dir: Path, phase: str):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "    dfs = []\n",
    "    for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "        chunk = cudf.read_parquet(path)\n",
    "        chunk[\"session\"] = chunk[\"session\"].astype(\"int32\")\n",
    "        chunk[\"aid\"] = chunk[\"aid\"].astype(\"int32\")\n",
    "        chunk[\"ts\"] = (chunk[\"ts\"] / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(type_labels).astype(\"int8\")\n",
    "        dfs.append(chunk)\n",
    "    \n",
    "    return cudf.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_gpu_memory(cmd_path=\"nvidia-smi\",\n",
    "                   target_properties=(\"memory.total\", \"memory.used\")):\n",
    "    \"\"\"\n",
    "    ref: https://www.12-technology.com/2022/01/pythongpu.html\n",
    "    Returns\n",
    "    -------\n",
    "    gpu_total : ndarray,  \"memory.total\"\n",
    "    gpu_used: ndarray, \"memory.used\"\n",
    "    \"\"\"\n",
    "\n",
    "    # format option\n",
    "    format_option = \"--format=csv,noheader,nounits\"\n",
    "\n",
    "    cmd = '%s --query-gpu=%s %s' % (cmd_path, ','.join(target_properties), format_option)\n",
    "\n",
    "    # Command execution in sub-processes\n",
    "    cmd_res = subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "    gpu_lines = cmd_res.decode().split('\\n')[0].split(', ')\n",
    "\n",
    "    gpu_total = int(gpu_lines[0]) / 1024\n",
    "    gpu_used = int(gpu_lines[1]) / 1024\n",
    "\n",
    "    gpu_total = np.round(gpu_used, 1)\n",
    "    gpu_used = np.round(gpu_used, 1)\n",
    "    return gpu_total, gpu_used\n",
    "\n",
    "\n",
    "class Trace():\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    @contextmanager\n",
    "    def timer(self, title):\n",
    "        t0 = time.time()\n",
    "        p = psutil.Process(os.getpid())\n",
    "        cpu_m0 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m0 = get_gpu_memory()[0]\n",
    "        yield\n",
    "        cpu_m1 = p.memory_info().rss / 2. ** 30\n",
    "        if self.cuda: gpu_m1 = get_gpu_memory()[0]\n",
    "\n",
    "        cpu_delta = cpu_m1 - cpu_m0\n",
    "        if self.cuda: gpu_delta = gpu_m1 - gpu_m0\n",
    "\n",
    "        cpu_sign = '+' if cpu_delta >= 0 else '-'\n",
    "        cpu_delta = math.fabs(cpu_delta)\n",
    "\n",
    "        if self.cuda: gpu_sign = '+' if gpu_delta >= 0 else '-'\n",
    "        if self.cuda: gpu_delta = math.fabs(gpu_delta)\n",
    "\n",
    "        cpu_message = f'{cpu_m1:.1f}GB({cpu_sign}{cpu_delta:.1f}GB)'\n",
    "        if self.cuda: gpu_message = f'{gpu_m1:.1f}GB({gpu_sign}{gpu_delta:.1f}GB)'\n",
    "\n",
    "        if self.cuda:\n",
    "            message = f\"[cpu: {cpu_message}, gpu: {gpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "        else:\n",
    "            message = f\"[cpu: {cpu_message}: {time.time() - t0:.1f}sec] {title} \"\n",
    "\n",
    "        print(message, file=sys.stderr)\n",
    "        \n",
    "trace = Trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49523d66-107d-485c-baff-bf96e19c1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Candidate features\n",
    "# =================================\n",
    "def read_file(f):\n",
    "    type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    df = cudf.read_parquet(f)\n",
    "    df[\"session\"] = df[\"session\"].astype(\"int32\")\n",
    "    df[\"aid\"] = df[\"aid\"].astype(\"int32\")\n",
    "    df[\"ts\"] = (df[\"ts\"] / 1000).astype(\"int32\")\n",
    "    df[\"type\"] = df[\"type\"].map(type_labels).astype(\"int8\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_type_weighted_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    type_weight = {0: 1, 1: 6, 2: 3}\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df[\"n\"] = df.groupby(\"session\").cumcount()\n",
    "                df = df.loc[df[\"n\"] < 30].drop(\"n\",axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on=\"session\")\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part * SIZE) & (df.aid_x < (part+1) * SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values([\"aid_x\", \"wgt\"], ascending=[True, False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp[\"n\"] = tmp.groupby(\"aid_x\").aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp[\"n\"] < 15].drop(\"n\", axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_buy2buy_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.loc[df[\"type\"].isin([1, 2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)] # 14days\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df['wgt'] = 1\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_15_buy2buy_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "def get_clicks_co_visitation_matrix():\n",
    "    files = sorted(list(Config.INPUT_DIR.glob(\"*_parquet/*.parquet\"))) # test set leakを使う\n",
    "    CHUNK = int(np.ceil(len(files)/6))\n",
    "    READ_CT = 5\n",
    "    # ref: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    # OOM回避のため共起行列を分割して計算・保存\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6 / DISK_PIECES\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for part in range(DISK_PIECES):\n",
    "        print('### DISK PART', part+1)\n",
    "        \n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # 共起行列を2層のチャンクで計算\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(6):\n",
    "            a = j * CHUNK\n",
    "            b = min((j+1) * CHUNK, len(files))\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "            \n",
    "            # => INNER CHUNKS\n",
    "            # CHUNKを更に分割して計算\n",
    "            for k in range(a, b, READ_CT):\n",
    "                # READ FILE\n",
    "                dfs = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k+i < b:\n",
    "                        dfs.append(read_file(files[k+i]))\n",
    "                \n",
    "                df = cudf.concat(dfs, ignore_index=True, axis=0)\n",
    "                df = df.sort_values(by=[\"session\", \"ts\"], ascending=[True, False]) # cumcountで最新30行を抽出するためにsessionを降順にしている\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df, on='session')\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= part*SIZE) & (df.aid_x < (part+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', \"ts_x\", 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', \"type_y\"])\n",
    "                df[\"wgt\"] = 1 + 3 * (df[\"ts_x\"] - 1659304800) / (1662328791 - 1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a:\n",
    "                    tmp2 = df\n",
    "                else:\n",
    "                    tmp2 = tmp2.add(df, fill_value=0)\n",
    "                    \n",
    "             # COMBINE OUTER CHUNKS\n",
    "            if a==0:\n",
    "                tmp = tmp2\n",
    "            else:\n",
    "                tmp = tmp.add(tmp2, fill_value=0)\n",
    "            \n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 15\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp[\"n\"] < 20].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        tmp.to_pandas().to_parquet(Config.CANDIDATE_FEATURE_DIR / f\"top_20_clicks_{part}.pqt\")\n",
    "        \n",
    "    del tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762edc17-3c35-4c18-b9d9-a80ecd7be1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import polars as pl\n",
    "# def get_input_data(input_dir: Path, phase: str):\n",
    "#     type_labels = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n",
    "    \n",
    "#     dfs = []\n",
    "#     for path in sorted(list(input_dir.glob(f\"{phase}_parquet/*.parquet\"))):\n",
    "#         chunk = pl.read_parquet(path)\n",
    "#         chunk = chunk.with_columns([\n",
    "#             pl.col(\"session\").cast(pl.Int32),\n",
    "#             pl.col(\"aid\").cast(pl.Int32),\n",
    "#             (pl.col(\"ts\") / 1000).cast(pl.Int32),\n",
    "#             pl.col(\"type\").apply(lambda x: type_labels[x]).cast(pl.Int32)\n",
    "#         ])\n",
    "#         dfs.append(chunk)\n",
    "    \n",
    "#     return pl.concat(dfs)\n",
    "\n",
    "\n",
    "# train_df = get_input_data(Config.INPUT_DIR, \"train\")\n",
    "# train_df.with_column((pl.col(\"aid\").cast(str) + \"_\" +  pl.col(\"type\").cast(str)).alias(\"aid_type\"))\n",
    "# df = train_df.groupby([\"session\"]).agg(pl.col(\"aid\"))[\"aid\"].to_list()\n",
    "\n",
    "# with trace.timer(\"W2V\"):\n",
    "#     w2vec = Word2Vec(sentences=df, vector_size=100, window=5, min_count=1, workers=-1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641c749e-b31e-4921-a5c6-478218bd14de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.4GB(+0.9GB), gpu: 0.6GB(+0.4GB): 153.7sec] cart_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"cart_co_matirx\"):\n",
    "    get_type_weighted_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b736689c-9515-43ed-8106-cb56bd292d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.4GB(+0.0GB), gpu: 0.6GB(+0.0GB): 31.4sec] buy2buy_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"buy2buy_co_matirx\"):\n",
    "    get_buy2buy_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ced086d-c1c8-49fa-a9c3-37f41c784b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "Processing files 125 thru 145 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.5GB(+0.0GB), gpu: 0.6GB(+0.0GB): 150.1sec] clicks_co_matirx \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"clicks_co_matirx\"):\n",
    "    get_clicks_co_visitation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103e4ce-69de-4785-8630-8a8461e71823",
   "metadata": {},
   "source": [
    "### Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f05bf36a-8467-4cee-9ff2-b5007a8ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def parquet2dict(path: Path) -> dict:\n",
    "    df = pd.read_parquet(path)\n",
    "    return df.groupby([\"aid_x\"])[\"aid_y\"].apply(list).to_dict()\n",
    "    # return df.groupby([\"aid_x\"]).parallel_apply(lambda x: x[\"aid_y\"].to_list()).to_dict()\n",
    "\n",
    "    \n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter()\n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    aids4 = [aid for aid in list(top_clicks) if aid not in result]\n",
    "    return result + aids4[:20 - len(result)]\n",
    "\n",
    "\n",
    "def suggest_carts(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']==0) | (df['type']==1)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_buys if aid in top_15_carts2orders]))\n",
    "        for aid in aids3:\n",
    "            aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_buys if aid in top_15_carts2orders]))\n",
    "    # aids4 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    aids4 = [aid for aid in list(top_carts) if aid not in result]\n",
    "    return result + aids4[:20 - len(result)]\n",
    "\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids = df[\"aid\"].to_list()\n",
    "    types = df[\"type\"].to_list()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']==1) | (df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "        for aid in aids3:\n",
    "            aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        \n",
    "        return sorted_aids\n",
    "    \n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_15_carts2orders[aid] for aid in unique_aids if aid in top_15_carts2orders]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_15_buy2buy[aid] for aid in unique_buys if aid in top_15_buy2buy]))\n",
    "    aids5 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3 + aids5).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    aids4 = [aid for aid in list(top_orders) if aid not in result]\n",
    "    return result + aids4[:20 - len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a991f4-a5b3-4123-a1d5-023d8ee88498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 1.6GB(+1.0GB), gpu: 0.6GB(+0.4GB): 4.1sec] load test set \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load test set\"):\n",
    "    test_df = get_input_data(Config.INPUT_DIR, \"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7be783-02b8-46ee-8b42-64e1f9520998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 5.2GB(+3.6GB), gpu: 0.6GB(+0.0GB): 71.8sec] load candidate feature \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"load candidate feature\"):\n",
    "    # type_weighted\n",
    "    top_15_carts2orders = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_carts_orders_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_15_carts2orders.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_15_carts_orders_{i}.pqt\"))\n",
    "    \n",
    "    # buy2buy\n",
    "    top_15_buy2buy = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_15_buy2buy_0.pqt\")\n",
    "    \n",
    "    # clicks\n",
    "    top_20_clicks = parquet2dict(Config.CANDIDATE_FEATURE_DIR / \"top_20_clicks_0.pqt\")\n",
    "    for i in range(1, Config.DISK_PIECES):\n",
    "        top_20_clicks.update(parquet2dict(Config.CANDIDATE_FEATURE_DIR / f\"top_20_clicks_{i}.pqt\"))\n",
    "        \n",
    "    top_clicks = test_df.loc[test_df[\"type\"] == 0, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    top_carts = test_df.loc[test_df[\"type\"] == 1, \"aid\"].value_counts().index.to_numpy()[:20]\n",
    "    top_orders = test_df.loc[test_df[\"type\"] == 2, \"aid\"].value_counts().index.to_numpy()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5bc261d-76f8-4f89-948c-28d72dec56c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 11.0GB(+0.6GB), gpu: 0.6GB(+0.0GB): 151.2sec] parallel apply \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"parallel apply\"):\n",
    "    pred_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "921c2882-6ddc-4d54-924e-0f402dc8156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 11.0GB(+0.0GB), gpu: 0.6GB(+0.0GB): 762.5sec] parallel apply \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"parallel apply\"):\n",
    "    pred_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_carts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "716439d1-6876-4663-b29d-dae525ce73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 12.6GB(+0.6GB), gpu: 0.6GB(+0.0GB): 732.3sec] parallel apply \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"parallel apply\"):\n",
    "    pred_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(suggest_buys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b678d5fa-8e1a-4c5c-8d41-e7b3cf84a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 13.4GB(+0.8GB), gpu: 0.6GB(+0.0GB): 45.8sec] make pred_df \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 571762 884502 876129 1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1339838 295362 1544564 441348 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 963957 254154 583026 364...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 1202618 1159379 77906 17040...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  11098528_clicks  11830 588923 1732105 571762 884502 876129 1157...\n",
       "1  11098529_clicks  1105029 459126 1339838 295362 1544564 441348 5...\n",
       "2  11098530_clicks  409236 264500 1603001 963957 254154 583026 364...\n",
       "3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n",
       "4  11098532_clicks  876469 7651 108125 1202618 1159379 77906 17040..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with trace.timer(\"make pred_df\"):\n",
    "    pred_clicks_df = pd.DataFrame(pred_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_carts_df = pd.DataFrame(pred_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_orders_df = pd.DataFrame(pred_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "    pred_df = pd.concat([pred_clicks_df, pred_carts_df, pred_orders_df], axis=0).reset_index(drop=True)\n",
    "    pred_df.columns = [\"session_type\", \"labels\"]\n",
    "    pred_df[\"labels\"] = pred_df[\"labels\"].apply(lambda x: \" \".join(map(str, x)))\n",
    "    pred_df.to_csv(\n",
    "        Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_validation.csv\" if Config.VALIDATION else Config.OUTPUT_DIR / f\"exp{Config.EXP_ID}_sub.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1578cee-736d-474d-b70a-3a14e222c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/4138111220.py:1: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  pd.DataFrame(pred_clicks[:100], columns=[\"labels\"]).explode(\"labels\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11098528</th>\n",
       "      <td>11830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098528</th>\n",
       "      <td>588923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098528</th>\n",
       "      <td>1732105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098528</th>\n",
       "      <td>571762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098528</th>\n",
       "      <td>884502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098627</th>\n",
       "      <td>987685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098627</th>\n",
       "      <td>889077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098627</th>\n",
       "      <td>1181342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098627</th>\n",
       "      <td>1595379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098627</th>\n",
       "      <td>777491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           labels\n",
       "session          \n",
       "11098528    11830\n",
       "11098528   588923\n",
       "11098528  1732105\n",
       "11098528   571762\n",
       "11098528   884502\n",
       "...           ...\n",
       "11098627   987685\n",
       "11098627   889077\n",
       "11098627  1181342\n",
       "11098627  1595379\n",
       "11098627   777491\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred_clicks[:100], columns=[\"labels\"]).explode(\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de360c3a-10f7-4f9a-8e02-a9a6e3f7592e",
   "metadata": {},
   "source": [
    "### validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa6ac19-4336-4cb6-9db8-3f2e74f51327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 6.7GB(-1.8GB), gpu: 0.6GB(+0.0GB): 3.8sec] delete temp file \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"delete temp file\"):\n",
    "    del test_df\n",
    "    del top_15_carts2orders, top_15_buy2buy, top_20_clicks, top_clicks, top_carts, top_orders\n",
    "    del pred_clicks, pred_buys\n",
    "    del pred_clicks_df, pred_carts_df, pred_orders_df\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8596a73d-fe5f-4fe7-921c-1800d2ab82c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5232806357769533\n",
      "carts recall = 0.40939401701097855\n",
      "orders recall = 0.6521574114279282\n",
      "=============\n",
      "Overall Recall = 0.5664407155377458\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cpu: 14.1GB(+0.7GB), gpu: 0.6GB(+0.0GB): 71.8sec] compute cv score \n"
     ]
    }
   ],
   "source": [
    "with trace.timer(\"compute cv score\"):\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet(Config.INPUT_DIR / \"test_labels.parquet\")\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "    \n",
    "    del sub, test_labels, recall, score\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088aa340-ffb4-4482-935e-a72732d230bb",
   "metadata": {},
   "source": [
    "1. 公開notebook通り\n",
    "- clicks recall = 0.5232863896228239\n",
    "- carts recall = 0.4094081486650003\n",
    "- orders recall = 0.6517810906868657\n",
    "- Overall Recall = 0.5662197379739019\n",
    "\n",
    "2. suggest_carts変更\n",
    "- clicks recall = 0.5232806357769533\n",
    "- carts recall = 0.4099009900990099\n",
    "- orders recall = 0.6517810906868657\n",
    "- Overall Recall = 0.5663670150195177\n",
    "\n",
    "3. suggest_carts変更\n",
    "- clicks recall = 0.5232806357769533\n",
    "- carts recall = 0.41061817154061525\n",
    "- orders recall = 0.6517810906868657\n",
    "- Overall Recall = 0.5665821694519992\n",
    "\n",
    "4. suggest_carts変更\n",
    "- 最後にtop_clicks etcをaddする際に重複するaidを除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10c7d-7b50-467a-a071-d101359aa54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
